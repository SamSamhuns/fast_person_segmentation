{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibyVrOTSwF56"
   },
   "source": [
    "**Install ONNX and ONNX Simplifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mI5aZkrXe3dV"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get install protobuf-compiler libprotoc-dev\n",
    "!pip install onnx\n",
    "!pip install onnx-simplifier\n",
    "!pip install torch-summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuFAltjx7CNs"
   },
   "source": [
    "**Load And Convert The Model to ONNX Format**\n",
    "\n",
    "Copy the model checkpoint weights(**model_296.pth**) and model class file(**SINet.py**) into the current directory, and finally export the pytorch model to onnx format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vr9RAMegvNFA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from SINet import *\n",
    "\n",
    "config = [[[3, 1], [5, 1]], [[3, 1], [3, 1]],\n",
    "              [[3, 1], [5, 1]], [[3, 1], [3, 1]], [[5, 1], [3, 2]], [[5, 2], [3, 4]],\n",
    "              [[3, 1], [3, 1]], [[5, 1], [5, 1]], [[3, 2], [3, 4]], [[3, 1], [5, 2]]]\n",
    "model = SINet(classes=2, p=2, q=8, config=config,\n",
    "                  chnn=1)\n",
    "\n",
    "model.load_state_dict(torch.load('/content/model_296.pth'))\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 320, 320)\n",
    "input_names = [ \"data\" ]\n",
    "output_names = [ \"classifier/1\" ]\n",
    "\n",
    "torch.onnx.export(model, dummy_input, \"SINet_320.onnx\", verbose=True, input_names=input_names, output_names=output_names, opset_version=11, export_params=True, do_constant_folding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hdxx6rhXQhDz"
   },
   "source": [
    "**Note:** The original  SINet class file was modified at line 116, i.e **x.size()** to **[int(s) for s in x.size()]** . This makes the size value static and prevents error during onnx conversion. Also, some layers like **ReduceMax** may not be fully supported in **opencv onnx** runtime, so replace this operation with appropriate reshape and **maxpool** layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T63K94u8RvBo"
   },
   "source": [
    "**Add Softmax Layer And Save Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjdwnJHaUHx9"
   },
   "source": [
    "Save another version of the model with **softmax output**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faYx1K2A5Kbk"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "soft_model = nn.Sequential(\n",
    "    model,\n",
    "    nn.Softmax(1)\n",
    ")\n",
    "\n",
    "# Export softmax model\n",
    "dummy_input = torch.randn(1, 3, 320, 320)\n",
    "\n",
    "soft_model.eval()\n",
    "input_names = [ \"data\" ]\n",
    "output_names = [ \"Softmax/1\" ]\n",
    "torch.onnx.export(soft_model, dummy_input, \"SINet_320_Softmax.onnx\", verbose=True, input_names=input_names, output_names=output_names, opset_version=11, export_params=True, do_constant_folding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNknGgqyQ3iO"
   },
   "source": [
    "**Optimize The Models With ONNX Simplifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enrrHg8_QZkU"
   },
   "outputs": [],
   "source": [
    "!python3 -m onnxsim SINet_320_Softmax.onnx SINet_320_optim_Softmax.onnx\n",
    "!python3 -m onnxsim SINet_320.onnx SINet_320_optim.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jVgVqdEOmN5t"
   },
   "source": [
    "**Run Inference Using ONNX-Runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pigJDwhUmUfO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import onnxruntime as rt\n",
    "\n",
    "img = cv2.imread('obama.jpg')\n",
    "img = cv2.resize(img, (320,320))\n",
    "img = img.astype(np.float32)\n",
    "\n",
    "# Preprocess images based on the original training/inference code\n",
    "mean = [102.890434, 111.25247,  126.91212 ]\n",
    "std = [62.93292,  62.82138,  66.355705]\n",
    "\n",
    "img=(img-mean)/std\n",
    "\n",
    "img /= 255\n",
    "img = img.transpose((2, 0, 1))\n",
    "img = img[np.newaxis,...]\n",
    "\n",
    "# Perform inference using the ONNX runtime\n",
    "sess = rt.InferenceSession(\"/content/SINet_320_optim.onnx\")\n",
    "input_name = sess.get_inputs()[0].name\n",
    "pred_onx = sess.run(None, {input_name: img.astype(np.float32)})[0]\n",
    "res=np.argmax(pred_onx[0], axis=0)[...,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1_US08dLyiz"
   },
   "source": [
    "Perform pytorch model inference on **GPU** and **compare** the results with **ONNX** model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1CYsThnLkUD"
   },
   "outputs": [],
   "source": [
    "# Enable gpu mode, if cuda available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and inputs into GPU\n",
    "model.to(device)\n",
    "inputs=torch.from_numpy(img).float().to(device)\n",
    "\n",
    "# Perform prediction and plot results\n",
    "with torch.no_grad():    \n",
    "         torch_res = model(inputs)\n",
    "         _, mask = torch.max(torch_res, 1)\n",
    "      \n",
    "torch_res = torch_res.cpu().numpy()\n",
    "\n",
    "# Compare the outputs of onnx and pytorch models\n",
    "np.allclose(pred_onx,torch_res, rtol=1e-03, atol=1e-05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtmi9OFrJhyH"
   },
   "source": [
    "**Note:-** \n",
    "* On a dual core 2.2GHz CPU, the inference time of ONNX model is about **0.064 seconds** (i.e **15 fps**) wihtout any additional optimizations. \n",
    "\n",
    "* On **Tesla T4 GPU**, the avg. execution time of the **pytorch model** was around 0.010s(**100 fps**), whereas on CPU it was around 0.144s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y8Y9Q0h_guZ"
   },
   "source": [
    "**Plot The Results Using Matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bi1eajsg8tFj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "\n",
    "# Read input and background images\n",
    "image = cv2.imread('obama.jpg')\n",
    "image = cv2.resize(image, (320,320))\n",
    "background =  cv2.imread('whitehouse.jpeg')\n",
    "background = cv2.resize(background, (320,320))\n",
    "\n",
    "# Crop image using mask and blend with background \n",
    "output = res*image + (1-res)*background\n",
    "output = cv2.cvtColor(output.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Plot the results using matplotlib\n",
    "im1 = cv2.cvtColor(image.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "im2 = cv2.cvtColor(background.astype(np.uint8), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "im3 = res.squeeze()*255\n",
    "im4 = output\n",
    "\n",
    "fig = plt.figure(figsize=(10., 10.))\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(2, 2),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.2,  # pad between axes in inch.\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, [im1, im2, im3, im4]):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.imshow(im)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75GxuhM0BYTM"
   },
   "source": [
    "**Pytorch To CoreML (Experimental)**\n",
    "\n",
    "Install latest coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2knBTmVpb7MA"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade coremltools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIBH_aGIBi0I"
   },
   "source": [
    "Load the pytorch saved model and directly convert in to **CoreML** format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ra60BZxictbr"
   },
   "outputs": [],
   "source": [
    "from SINet import *\n",
    "import coremltools as ct\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Load pytorch model\n",
    "config = [[[3, 1], [5, 1]], [[3, 1], [3, 1]],\n",
    "              [[3, 1], [5, 1]], [[3, 1], [3, 1]], [[5, 1], [3, 2]], [[5, 2], [3, 4]],\n",
    "              [[3, 1], [3, 1]], [[5, 1], [5, 1]], [[3, 2], [3, 4]], [[3, 1], [5, 2]]]\n",
    "model = SINet(classes=2, p=2, q=8, config=config,\n",
    "                  chnn=1)\n",
    "\n",
    "model.load_state_dict(torch.load('/content/model_296.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Get a pytorch model and save it as a *.pt file\n",
    "pytorch_model = model\n",
    "pytorch_model.eval()\n",
    "example_input = torch.rand(1, 3, 320, 320)\n",
    "traced_model = torch.jit.trace(pytorch_model, example_input)\n",
    "traced_model.save(\"sinet.pt\")\n",
    "\n",
    "# Convert the saved PyTorch model to Core ML\n",
    "mlmodel = ct.convert(\"sinet.pt\",\n",
    "                    inputs=[ct.TensorType(shape=(1, 3, 320, 320))])\n",
    "\n",
    "# Save the coreml model\n",
    "mlmodel.save(\"SINet.mlmodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CejXVr1RCJSG"
   },
   "source": [
    "**Note:** The converter seems to throw an error - '**raise ValueError(\"x should be at least rank 3\")** ' due to prelu activation dimension mismatch. However if we commnet out the lines 270-278( function: type_inference(self)) in the file: /usr/local/lib/python3.6/dist-packages/coremltools/converters/mil/mil/ops/defs/activation.py., the conversion was successful and the **mlmodel** was saved. Unfortunately we cannot test the model(prediction) on a linux system; it requires macOS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l_z2X8FELGB"
   },
   "source": [
    "**References:-**\n",
    "\n",
    "1.   https://github.com/onnx/onnx/blob/master/docs/PythonAPIOverview.md\n",
    "2.   https://github.com/daquexian/onnx-simplifier\n",
    "3. https://github.com/microsoft/onnxruntime/blob/master/docs/python/tutorial.rst#step-3-load-and-run-the-model-using-onnx-runtime\n",
    "4. https://github.com/clovaai/ext_portrait_segmentation\n",
    "5. https://coremltools.readme.io/docs/introductory-quickstart"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SINet_ONNX.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
